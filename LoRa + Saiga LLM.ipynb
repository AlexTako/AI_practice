{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f3d22fca6c0b4c3eb1c87c4b13314fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a63e05d782374be89c1705bb2f6f0d6f",
              "IPY_MODEL_05590d46e70044bd94a9c958be5f5c5b",
              "IPY_MODEL_e0e2d742050b4f688321eb51eb204340"
            ],
            "layout": "IPY_MODEL_529be4bc6a554e4eb24c86ae8e0d719e"
          }
        },
        "a63e05d782374be89c1705bb2f6f0d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58941560997a47129adbf71a9238665c",
            "placeholder": "​",
            "style": "IPY_MODEL_c2cf23926dc74821bf9a73169e79636f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "05590d46e70044bd94a9c958be5f5c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11284f87ae0e4c9bb2c97268a5ab04c8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e98f0241c834ec898b1b7d8c1bacfad",
            "value": 2
          }
        },
        "e0e2d742050b4f688321eb51eb204340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a070129bfe5141f3ae2457b08e2b53e2",
            "placeholder": "​",
            "style": "IPY_MODEL_d399be25f52844b99e1196074445e735",
            "value": " 2/2 [01:22&lt;00:00, 38.14s/it]"
          }
        },
        "529be4bc6a554e4eb24c86ae8e0d719e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58941560997a47129adbf71a9238665c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2cf23926dc74821bf9a73169e79636f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11284f87ae0e4c9bb2c97268a5ab04c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e98f0241c834ec898b1b7d8c1bacfad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a070129bfe5141f3ae2457b08e2b53e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d399be25f52844b99e1196074445e735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bb87eaffcda469bac813e6bb9248f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cb94eaf904d432391de71111489f8c6",
              "IPY_MODEL_3a6bb1d2d3414b888ab7d426194dcef5",
              "IPY_MODEL_2f0f5e8ece454a89a724107c3cded153"
            ],
            "layout": "IPY_MODEL_6c7b7bef1d3643d8aadfa20267d57c16"
          }
        },
        "9cb94eaf904d432391de71111489f8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d82a85a48814c18838c2179fe1a6850",
            "placeholder": "​",
            "style": "IPY_MODEL_eaaf9df7576d4e808f2e2bcc61a1c8f2",
            "value": "Generating train split: "
          }
        },
        "3a6bb1d2d3414b888ab7d426194dcef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_872f38d9f7e740a2a8e3ecfecae27dae",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecd9ccdffd6443a792adbbb47626f0b5",
            "value": 1
          }
        },
        "2f0f5e8ece454a89a724107c3cded153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b76230c46ba3433fad4fec12b8ba733f",
            "placeholder": "​",
            "style": "IPY_MODEL_7bba63778e104b06bd7dcf0f507bc548",
            "value": " 37731/0 [00:01&lt;00:00, 36055.24 examples/s]"
          }
        },
        "6c7b7bef1d3643d8aadfa20267d57c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d82a85a48814c18838c2179fe1a6850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaaf9df7576d4e808f2e2bcc61a1c8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "872f38d9f7e740a2a8e3ecfecae27dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ecd9ccdffd6443a792adbbb47626f0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b76230c46ba3433fad4fec12b8ba733f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bba63778e104b06bd7dcf0f507bc548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexTako/AI_practice/blob/main/LoRa%20%2B%20Saiga%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate evaluate bitsandbytes trl peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdAeW3xt1_Pi",
        "outputId": "5eefdcf2-f907-42fc-8e4c-cfa55cf3caee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.9.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n",
        "!wget https://raw.githubusercontent.com/IlyaGusev/rulm/master/self_instruct/src/interact_mistral_llamacpp.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVYCKRMvJjxX",
        "outputId": "b2380c4a-bfe2-4030-971e-bb695ffd59d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-13 12:14:55--  https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.102.6, 3.165.102.58, 3.165.102.128, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.102.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1721132095&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMTEzMjA5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=d1BcNkNHKcEg2AWBLXYzq8xOL0riLUZyT3e45b9Ias7rVHl69xHks35aC8CzQZh-AiBFvxiGPRPNXp3n2bEH0uR-eOgcYt5FIHwAEkSP2%7ENWo37Vx-bRQeAbnMM4BgimXIwqnkv7GnyaviZSzpKFJS-xVw3ApAxnFq4tfllX4yKEa7Myui7EVhfYBbIiOMUAn7oU2V7djR5cq8UPFaCSuwgQga%7EXon61zWLU-oNqtMN7ySrUguWd9OBOlb27zIfh0qr9L4gnX49LhrD%7EaPcMAszcfwlRhF-rDgOKukLuys4Kls7Iqjg9Yo3Iw04DnmAQvDxZBQE4vrggwjsdmfrEGw__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-07-13 12:14:55--  https://cdn-lfs.huggingface.co/repos/79/b3/79b3fc4694b2c3a22273003a1de570f145c14f0586c212c28c28e302adf5d3d6/2798f33ff63c791a21f05c1ee9a10bc95630b17225c140c197188a3d5cf32644?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1721132095&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMTEzMjA5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83OS9iMy83OWIzZmM0Njk0YjJjM2EyMjI3MzAwM2ExZGU1NzBmMTQ1YzE0ZjA1ODZjMjEyYzI4YzI4ZTMwMmFkZjVkM2Q2LzI3OThmMzNmZjYzYzc5MWEyMWYwNWMxZWU5YTEwYmM5NTYzMGIxNzIyNWMxNDBjMTk3MTg4YTNkNWNmMzI2NDQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=d1BcNkNHKcEg2AWBLXYzq8xOL0riLUZyT3e45b9Ias7rVHl69xHks35aC8CzQZh-AiBFvxiGPRPNXp3n2bEH0uR-eOgcYt5FIHwAEkSP2%7ENWo37Vx-bRQeAbnMM4BgimXIwqnkv7GnyaviZSzpKFJS-xVw3ApAxnFq4tfllX4yKEa7Myui7EVhfYBbIiOMUAn7oU2V7djR5cq8UPFaCSuwgQga%7EXon61zWLU-oNqtMN7ySrUguWd9OBOlb27zIfh0qr9L4gnX49LhrD%7EaPcMAszcfwlRhF-rDgOKukLuys4Kls7Iqjg9Yo3Iw04DnmAQvDxZBQE4vrggwjsdmfrEGw__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.156.133.51, 108.156.133.103, 108.156.133.68, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.156.133.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368450336 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘model-q4_K.gguf.1’\n",
            "\n",
            "model-q4_K.gguf.1   100%[===================>]   4.07G  60.2MB/s    in 46s     \n",
            "\n",
            "2024-07-13 12:15:41 (91.0 MB/s) - ‘model-q4_K.gguf.1’ saved [4368450336/4368450336]\n",
            "\n",
            "--2024-07-13 12:15:41--  https://raw.githubusercontent.com/IlyaGusev/rulm/master/self_instruct/src/interact_mistral_llamacpp.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1774 (1.7K) [text/plain]\n",
            "Saving to: ‘interact_mistral_llamacpp.py.1’\n",
            "\n",
            "interact_mistral_ll 100%[===================>]   1.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-13 12:15:41 (29.2 MB/s) - ‘interact_mistral_llamacpp.py.1’ saved [1774/1774]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U_Q3qjet4R5",
        "outputId": "e01cfc01-7f65-4bb1-b171-2370aa104d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.82)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 interact_mistral_llamacpp.py model-q4_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxlvGM84u6CN",
        "outputId": "ea356437-bf97-4dc3-c010-bd5743ab45f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from model-q4_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 261\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = models\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2016\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   252.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  252.00 MiB, K (f16):  126.00 MiB, V (f16):  126.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   161.94 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n",
            "User: Traceback (most recent call last):\n",
            "  File \"/content/interact_mistral_llamacpp.py\", line 63, in <module>\n",
            "    fire.Fire(interact)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 143, in Fire\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 477, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/interact_mistral_llamacpp.py\", line 41, in interact\n",
            "    user_message = input(\"User: \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-MFrqXyEOzk",
        "outputId": "31df9c08-10d1-4d93-9f9c-86a3c417ba0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kza46waHN4_",
        "outputId": "6ba116cc-356c-4afa-cc08-268068fa6bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Идентификатор адаптера, так как мы выбрали push_to_hub=True в аргументах обучения,\n",
        "# то модель по токену загрузилась на HuggingFace\n",
        "peft_model_id = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
        "\n",
        "# Базовая модель, к которой мы обучили адаптер\n",
        "base_model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
        "\n",
        "# # Загрузка модели с PEFT адаптером\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     peft_model_id,      # адаптер\n",
        "#     device_map={\"\": 0}, # использовать GPU:0, трюк если auto приводит к ошибке\n",
        "#     torch_dtype=torch.float16\n",
        "# )\n",
        "\n",
        "# # Загрузка модели с PEFT адаптером с использованием частичной загрузки\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     peft_model_id,\n",
        "#     device_map={\"\": 0, \"layer.0\": \"cpu\"},  # частичная загрузка на GPU и CPU\n",
        "#     torch_dtype=torch.float16\n",
        "# )\n",
        "\n",
        "# Конфигурация квантования\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "# Загрузка модели с PEFT адаптером и конфигурацией квантования\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0, \"layer.0\": \"cpu\"} # частичная загрузка на GPU и CPU\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_name,          # базовая модель\n",
        "    trust_remote_code=True)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/ru_turbo_saiga.jsonl\", split=\"train\")\n",
        "rand_idx = randint(0, len(eval_dataset))\n",
        "\n",
        "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=120,\n",
        "               temperature=0.2, top_p=0.95, do_sample=True,\n",
        "               eos_token_id=pipe.tokenizer.eos_token_id,\n",
        "               pad_token_id=pipe.tokenizer.pad_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "f3d22fca6c0b4c3eb1c87c4b13314fd6",
            "a63e05d782374be89c1705bb2f6f0d6f",
            "05590d46e70044bd94a9c958be5f5c5b",
            "e0e2d742050b4f688321eb51eb204340",
            "529be4bc6a554e4eb24c86ae8e0d719e",
            "58941560997a47129adbf71a9238665c",
            "c2cf23926dc74821bf9a73169e79636f",
            "11284f87ae0e4c9bb2c97268a5ab04c8",
            "2e98f0241c834ec898b1b7d8c1bacfad",
            "a070129bfe5141f3ae2457b08e2b53e2",
            "d399be25f52844b99e1196074445e735",
            "6bb87eaffcda469bac813e6bb9248f08",
            "9cb94eaf904d432391de71111489f8c6",
            "3a6bb1d2d3414b888ab7d426194dcef5",
            "2f0f5e8ece454a89a724107c3cded153",
            "6c7b7bef1d3643d8aadfa20267d57c16",
            "8d82a85a48814c18838c2179fe1a6850",
            "eaaf9df7576d4e808f2e2bcc61a1c8f2",
            "872f38d9f7e740a2a8e3ecfecae27dae",
            "ecd9ccdffd6443a792adbbb47626f0b5",
            "b76230c46ba3433fad4fec12b8ba733f",
            "7bba63778e104b06bd7dcf0f507bc548"
          ]
        },
        "id": "rmY2GW87Gm0v",
        "outputId": "3f64a6ad-4bea-49cf-b2a0-4015b9dde25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3d22fca6c0b4c3eb1c87c4b13314fd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bb87eaffcda469bac813e6bb9248f08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Мы решили, что доверяем именно этой модели\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL3mEUknvoq1",
        "outputId": "96388be0-a1ca-41b4-f4b6-554b6c1922ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизируем строки\n",
        "request= model.encode(eval_dataset[rand_idx]['messages'][1]['content'], convert_to_tensor=True)\n",
        "answer = model.encode(eval_dataset[rand_idx]['messages'][2]['content'], convert_to_tensor=True)\n",
        "predict= model.encode(outputs[0]['generated_text'][len(prompt):].strip(), convert_to_tensor=True)\n",
        "\n",
        "# Расчитываем косинусное сходство\n",
        "print(util.pytorch_cos_sim(request, answer))\n",
        "print(util.pytorch_cos_sim(request, predict))\n",
        "print(util.pytorch_cos_sim(answer, predict))"
      ],
      "metadata": {
        "id": "NhLwUhjiF_Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b314d96-8b5e-4337-c95c-77ed8312a25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4580]], device='cuda:0')\n",
            "tensor([[0.5455]], device='cuda:0')\n",
            "tensor([[0.3607]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # для отрисовки прогресс бара выполнения\n",
        "\n",
        "# Функция оценки образца\n",
        "def evaluate(sample):\n",
        "    # Промпт для образца\n",
        "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
        "    # Генерируем ответ\n",
        "    outputs = pipe(prompt, max_new_tokens=120,\n",
        "               temperature=0.2, top_p=0.95, do_sample=True,\n",
        "               eos_token_id=pipe.tokenizer.eos_token_id,\n",
        "               pad_token_id=pipe.tokenizer.pad_token_id)\n",
        "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    # Токенизируем истинный и предсказанный ответы\n",
        "    answer = model.encode(sample[\"messages\"][2][\"content\"], convert_to_tensor=True)\n",
        "    predict= model.encode(predicted_answer, convert_to_tensor=True)\n",
        "\n",
        "    # Вычисляем косинусное сходство между истинным и предсказанным ответом\n",
        "    if float(util.pytorch_cos_sim(answer, predict))>0.5:\n",
        "        # Выше порога - плюс в карму точности\n",
        "        return 1\n",
        "    else:\n",
        "        # Ниже порога - минус в карму точности\n",
        "        return 0\n",
        "\n",
        "# Массив с оценками\n",
        "success_rate = []\n",
        "number_of_eval_samples = 100\n",
        "\n",
        "# Отбираем 100 записей и делаем по ним цикл с отображением прогресс бара\n",
        "for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
        "    # Результаты оценки складываем в массив\n",
        "    success_rate.append(evaluate(s))\n",
        "\n",
        "# Вычисление точности\n",
        "accuracy = sum(success_rate)/len(success_rate)\n",
        "\n",
        "print(f\"Точность: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "4NCJuq0AGFkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363d6c4b-6842-453b-a432-bf57cbc782fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  1%|          | 1/100 [00:25<41:18, 25.03s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  2%|▏         | 2/100 [00:39<31:06, 19.04s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  3%|▎         | 3/100 [00:54<27:43, 17.15s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  4%|▍         | 4/100 [01:09<25:47, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  5%|▌         | 5/100 [01:25<25:39, 16.21s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  6%|▌         | 6/100 [01:40<24:26, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  7%|▋         | 7/100 [01:54<23:38, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  8%|▊         | 8/100 [02:09<23:05, 15.06s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "  9%|▉         | 9/100 [02:23<22:28, 14.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 10%|█         | 10/100 [02:37<21:59, 14.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 11%|█         | 11/100 [02:52<21:35, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 12%|█▏        | 12/100 [03:07<21:28, 14.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 13%|█▎        | 13/100 [03:21<21:13, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 14%|█▍        | 14/100 [03:36<20:51, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 15%|█▌        | 15/100 [03:50<20:30, 14.48s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 16%|█▌        | 16/100 [04:04<20:19, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 17%|█▋        | 17/100 [04:19<20:09, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 18%|█▊        | 18/100 [04:34<19:50, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 19%|█▉        | 19/100 [04:48<19:35, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 20%|██        | 20/100 [05:02<19:18, 14.48s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 21%|██        | 21/100 [05:17<19:09, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 22%|██▏       | 22/100 [05:32<19:07, 14.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 23%|██▎       | 23/100 [05:47<18:55, 14.75s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 24%|██▍       | 24/100 [06:02<18:37, 14.70s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 25%|██▌       | 25/100 [06:16<18:16, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 26%|██▌       | 26/100 [06:30<17:56, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 27%|██▋       | 27/100 [06:45<17:38, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 28%|██▊       | 28/100 [07:00<17:27, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 29%|██▉       | 29/100 [07:14<17:13, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 30%|███       | 30/100 [07:29<16:58, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 31%|███       | 31/100 [07:43<16:41, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 32%|███▏      | 32/100 [07:58<16:34, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 33%|███▎      | 33/100 [08:13<16:21, 14.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 34%|███▍      | 34/100 [08:27<16:03, 14.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 35%|███▌      | 35/100 [08:41<15:42, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 36%|███▌      | 36/100 [08:56<15:28, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 37%|███▋      | 37/100 [09:10<15:12, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 38%|███▊      | 38/100 [09:25<14:59, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 39%|███▉      | 39/100 [09:39<14:46, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 40%|████      | 40/100 [09:54<14:31, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 41%|████      | 41/100 [10:09<14:21, 14.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 42%|████▏     | 42/100 [10:24<14:10, 14.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 43%|████▎     | 43/100 [10:38<13:59, 14.74s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 44%|████▍     | 44/100 [10:53<13:40, 14.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 45%|████▌     | 45/100 [11:07<13:23, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 46%|████▌     | 46/100 [11:22<13:09, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 47%|████▋     | 47/100 [11:37<12:55, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 48%|████▊     | 48/100 [11:51<12:37, 14.58s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 49%|████▉     | 49/100 [12:06<12:20, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 50%|█████     | 50/100 [12:20<12:03, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 51%|█████     | 51/100 [12:35<11:50, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 52%|█████▏    | 52/100 [12:50<11:46, 14.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 53%|█████▎    | 53/100 [13:05<11:36, 14.82s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 54%|█████▍    | 54/100 [13:19<11:18, 14.74s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 55%|█████▌    | 55/100 [13:34<11:00, 14.68s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 56%|█████▌    | 56/100 [13:48<10:44, 14.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 57%|█████▋    | 57/100 [14:03<10:27, 14.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 58%|█████▊    | 58/100 [14:17<10:10, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 59%|█████▉    | 59/100 [14:32<09:55, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 60%|██████    | 60/100 [14:47<09:42, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 61%|██████    | 61/100 [15:01<09:27, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 62%|██████▏   | 62/100 [15:16<09:17, 14.68s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 63%|██████▎   | 63/100 [15:31<09:03, 14.69s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 64%|██████▍   | 64/100 [15:45<08:43, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 65%|██████▌   | 65/100 [16:00<08:29, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 66%|██████▌   | 66/100 [16:14<08:14, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 67%|██████▋   | 67/100 [16:29<07:59, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 68%|██████▊   | 68/100 [16:43<07:43, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 69%|██████▉   | 69/100 [16:57<07:24, 14.35s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 70%|███████   | 70/100 [17:11<07:11, 14.40s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 71%|███████   | 71/100 [17:26<06:59, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 72%|███████▏  | 72/100 [17:41<06:50, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 73%|███████▎  | 73/100 [17:56<06:35, 14.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 74%|███████▍  | 74/100 [18:10<06:19, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 75%|███████▌  | 75/100 [18:25<06:04, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 76%|███████▌  | 76/100 [18:39<05:47, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 77%|███████▋  | 77/100 [18:53<05:32, 14.44s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 78%|███████▊  | 78/100 [19:08<05:17, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 79%|███████▉  | 79/100 [19:22<05:03, 14.44s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 80%|████████  | 80/100 [19:37<04:48, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 81%|████████  | 81/100 [19:52<04:37, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 82%|████████▏ | 82/100 [20:07<04:25, 14.77s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 83%|████████▎ | 83/100 [20:21<04:10, 14.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 84%|████████▍ | 84/100 [20:36<03:54, 14.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 85%|████████▌ | 85/100 [20:50<03:37, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 86%|████████▌ | 86/100 [21:05<03:23, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 87%|████████▋ | 87/100 [21:19<03:08, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 88%|████████▊ | 88/100 [21:34<02:54, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 89%|████████▉ | 89/100 [21:49<02:40, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 90%|█████████ | 90/100 [22:03<02:26, 14.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 91%|█████████ | 91/100 [22:18<02:12, 14.70s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 92%|█████████▏| 92/100 [22:33<01:58, 14.80s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 93%|█████████▎| 93/100 [22:48<01:43, 14.82s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 94%|█████████▍| 94/100 [23:03<01:28, 14.72s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 95%|█████████▌| 95/100 [23:17<01:13, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 96%|█████████▌| 96/100 [23:32<00:58, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 97%|█████████▋| 97/100 [23:46<00:43, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 98%|█████████▊| 98/100 [24:00<00:29, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            " 99%|█████████▉| 99/100 [24:15<00:14, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
            "100%|██████████| 100/100 [24:29<00:00, 14.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность: 17.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_accumulated_memory_stats()\n",
        "torch.cuda.reset_max_memory_cached()\n",
        "del model, pipe, tokenizer"
      ],
      "metadata": {
        "id": "57C9qiT02cnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}